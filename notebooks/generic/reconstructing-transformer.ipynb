{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing Transformer\n",
    "\n",
    "This notebook purpose is auto-didactic exploration of transformer code connected to equations from the paper.\n",
    "\n",
    "### Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance of encoder-decoder transformer is the default model in pytorch, it is constructed of encoder and decoder blocks, just like the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torch.nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation=\"relu\",\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "\n",
    "transformer = transformer.eval()  # turn off dropout etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has over 40 million parameters, which means that it is hard to train quickly, but it can be easily explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44140544"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in transformer.parameters() if param.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Encoder and Decoder are abstractions, consisting of multiple encoder and decoder blocks. It is possible to create instances of those blocks one by one, each of those blocks is governed by a single set of equations, which are the same for each layer, but with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: 18915328\n",
      "Decoder: 25225216\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encoder: {sum(param.numel() for param in transformer.encoder.parameters() if param.requires_grad)}\")\n",
    "print(f\"Decoder: {sum(param.numel() for param in transformer.decoder.parameters() if param.requires_grad)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder consist of encoder block with self-attention and feed-forward layer. Usually in NLP applications words are converted to tokens and to embeddings, which allow the model to process the input, in this implementation initial embedding is skipped, since transformer can be used to different data types, not only text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(8, 32, 512)  # shape = (BATCH_SIZE, SEQUENCE_LEN, EMBEDDING_DIM)  EMBEDDING_DIM also known as d_model or model dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing toy input (random tensor) does not change its change, when processed with encoder block or full encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 32, 512]), torch.Size([8, 32, 512]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    layer_outputs = transformer.encoder.layers[0](inputs)\n",
    "    encoder_outputs = transformer.encoder(inputs)\n",
    "\n",
    "layer_outputs.shape, encoder_outputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder has two sub-layers: Multi-Head Attention and Feed-Forward Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller encoder block\n",
    "encoder_block = torch.nn.TransformerEncoderLayer(\n",
    "    d_model=32,\n",
    "    nhead=1,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.0,\n",
    "    activation=\"relu\",\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention is computed first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 32]), torch.Size([1, 4, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.rand(1, 4, 32)  # one example with 4 items and model dimension of 32\n",
    "    outputs, scores = encoder_block.self_attn(query=x, key=x, value=x)  # in self attention Q, K, V are all the same\n",
    "\n",
    "outputs.shape, scores.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each learnable operation, there is a residual connection and a layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = x + outputs  # skip-connection\n",
    "    x = encoder_block.norm1(x)  # layer normalization\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-forward network is applied to the output of the self-attention sub-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = encoder_block.linear1(x)  # linear up-projection transformation\n",
    "    x = encoder_block.activation(x)  # activation\n",
    "    x = encoder_block.linear2(x)  # linear down-projection transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sub-layer has a residual connection around it followed by a layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = x + outputs  # skip-connection\n",
    "    x = encoder_block.norm1(x)  # layer normalization\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "Decoder has 3 sublayers, two are the same as in encoder and the third one is multi-head cross-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.decoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 32, 512]), torch.Size([8, 3, 512]), torch.Size([8, 3, 512]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.rand(8, 32, 512)\n",
    "target = torch.rand(8, 3, 512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = transformer.encoder(src=inputs)  # encoder output is called memory in pytorch\n",
    "    decoder_outputs = transformer.decoder(tgt=target, memory=encoder_outputs)\n",
    "    outputs = transformer(src=inputs, tgt=target)\n",
    "\n",
    "encoder_outputs.shape, decoder_outputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all masks are none by default so outputs are exactly the same \n",
    "torch.all(outputs == decoder_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During inference language transformers (for example for machine translation) are auto-regressive, which means they require multiple inference steps to produce the output. In this case the output of the previous step is used as an input to the next step. For language tasks decoder output needs to be converted to tokens, which is done by linear layer followed by softmax activation converting dense decoder output to probability over vocabulary.\n",
    "\n",
    "Decoder outputs representation for whole sequence, but in standard application only last token is used and appended to input sequence for the next step. Loop is broken when special token for sentence end is generated or when model reaches maximum number of steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller encoder block\n",
    "decoder_block = torch.nn.TransformerDecoderLayer(\n",
    "    d_model=32,\n",
    "    nhead=1,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.0,\n",
    "    activation=\"relu\",\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, x[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mshape, \u001b[39mlen\u001b[39m(x)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "x[0].shape, x[1].shape, len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 24, 32])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.rand(8, 32, 32)  # (BATCH_SIZE, SEQUENCE_LENGTH, MODEL_DIMENSION)\n",
    "encoder_outputs = torch.rand(8, 32, 32)\n",
    "target = torch.rand(8, 24, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # self-attention over target which is the decoder input\n",
    "    x, _ = decoder_block.self_attn(key=target, query=target, value=target)  # ignore attention weights\n",
    "    x = decoder_block.dropout1(x)  # dropout\n",
    "    self_attention_outputs = decoder_block.norm1(x + target)  # skip-connection and layer normalization\n",
    "    # cross-attention using encoder outputs as key and value\n",
    "    x, _ = decoder_block.multihead_attn(query=self_attention_outputs, key=encoder_outputs, value=encoder_outputs)\n",
    "    x = decoder_block.dropout2(x)\n",
    "    multihead_attention_outputs = decoder_block.norm2(x + self_attention_outputs)  # skip connection\n",
    "    # linear feed forward\n",
    "    x = decoder_block.linear1(multihead_attention_outputs)\n",
    "    x = decoder_block.activation(x)\n",
    "    x = decoder_block.linear2(x)\n",
    "    x = decoder_block.dropout3(x)\n",
    "    outputs = decoder_block.norm3(x + multihead_attention_outputs)  # skip connection\n",
    "    \n",
    "outputs.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Key building block of transformer model is multi-head attention. In classical implementation it is used both as self-attention and cross-attention, where the difference between self and cross attention is the inputs to the layer, while the underlying mechanism is the same.\n",
    "\n",
    "The multi-head attention mechanism has three inputs (with additional masks): \n",
    "* `query`\n",
    "* `key`\n",
    "* `value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://ai.stackexchange.com/questions/35548/when-exactly-does-the-split-into-different-heads-in-multi-head-attention-occur\n",
    "attention = torch.nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True, bias=False)\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "attention = attention.eval()  # turn off dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 32]), torch.Size([2, 16, 16]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 16, 32) \n",
    "\n",
    "outputs, scores = attention(x, x, x)\n",
    "outputs.shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight : torch.Size([96, 32])\n",
      "out_proj.weight : torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "for name, param in attention.named_parameters():\n",
    "    print(f\"{name} : {param.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is optimized so only single parameter matrix is create for query, key and value projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 32]), torch.Size([2, 16, 32]), torch.Size([2, 16, 32]))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention where query, key, value are all the same\n",
    "x = torch.rand(2, 16, 32)  # no batch\n",
    "\n",
    "q_proj_weight = attention.in_proj_weight[:32, :]  # query projection weight\n",
    "k_proj_weight = attention.in_proj_weight[32:64, :]  # key projection weight\n",
    "v_proj_weight = attention.in_proj_weight[64:, :]  # value projection weight\n",
    "\n",
    "# multiplication operation ignored batch dimension\n",
    "# those projections are allowed to have bias, but it is skipped for simplicity\n",
    "query = x @ q_proj_weight  # query projection is simply a matrix multiplication\n",
    "key = x @ k_proj_weight  # key can have different size than query and value\n",
    "value = x @ v_proj_weight\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those computed values are used to as input to attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 16])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ key.transpose(1, 2)  # this is a regular transpose ignoring batch dimension\n",
    "attention_scores = softmax(attention_scores / torch.sqrt(torch.Tensor([32])))  # element-wise with regularization\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 32])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = attention_scores @ value  # output computation\n",
    "outputs = outputs @ attention.out_proj.weight.transpose(0, 1)  # output projection required by multihead attention\n",
    "outputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
